{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwYv5vVwCXPD0aqjPOcYOo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalhotra18/HuggingFace/blob/main/Synthetic_Data_Generator_Product.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-07tpoDedLl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece accelerate bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "-gP3t4oSehw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "oNO-g_Q4eizD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model (choose from: mistralai/Mixtral-8x7B-Instruct-v0.1 or meta-llama/Meta-Llama-3.1-8B-Instruct)\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "Oag28_egekJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization config to load in 4-bit for speed and memory efficiency\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "Nr8DiA4AkHn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "kUzPDsWdkOL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main generation function\n",
        "def generate_synthetic_data(prompt, rows=5):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates synthetic data in structured formats (CSV/JSON/Markdown) for testing.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Please generate {rows} rows of synthetic data. {prompt}\"}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(inputs, max_new_tokens=1024)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"</s>\")[-1].strip()"
      ],
      "metadata": {
        "id": "4BfBnY2ckPmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "interface = gr.Interface(\n",
        "    fn=generate_synthetic_data,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Describe the Dataset (e.g. 'fake user data with name, email, and age')\"),\n",
        "        gr.Slider(1, 50, value=5, step=1, label=\"Number of Rows\")\n",
        "    ],\n",
        "    outputs=gr.Code(label=\"ðŸ§ª Synthetic Data Output\"),\n",
        "    title=\"Synthetic Data Generator\",\n",
        "    description=\"Describe the type of data you need and how many rows. This tool will generate realistic-looking synthetic data using open-source models.\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ba89E-ZhkR-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "TRhdDN7AkUQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}